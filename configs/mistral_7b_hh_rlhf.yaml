# Configuration for Dual-Head training on Mistral-7B with HH-RLHF
# Cross-architecture evaluation setup

model:
  backbone_name_or_path: "mistralai/Mistral-7B-v0.1"
  freeze_backbone: true
  
  # Head configuration (same as LLaMA for consistency)
  lm_bias: false
  rm_bias: false
  rm_intermediate_size: null
  rm_activation: "silu"
  rm_dropout: 0.1
  
  # Gating configuration
  gating_num_heads: 8
  gating_intermediate_size: null
  gating_activation: "gelu"
  gating_dropout: 0.1
  gating_use_layer_norm: true
  
  # Fusion configuration
  learnable_temperature: false
  initial_temperature: 1.0

training:
  # Loss weights (same as paper)
  lambda_r: 1.0
  lambda_g: 0.01
  beta_r: 1.0
  
  # Training parameters
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 4
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  
  # Optimization
  fp16: false
  bf16: true
  gradient_checkpointing: true
  dataloader_num_workers: 4
  
  # Logging and saving
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"

data:
  dataset_name: "Anthropic/hh-rlhf"
  dataset_config_name: null
  max_seq_length: 2048
  preprocessing_num_workers: 4
  training_mode: "preference"
  sft_ratio: 0.1

system:
  seed: 42
  output_dir: "./outputs/dual_head_mistral_7b_hh_rlhf"
  report_to: "wandb"
  project_name: "dual-head-iclr2026"
  run_name: "mistral_7b_hh_rlhf_cross_arch"